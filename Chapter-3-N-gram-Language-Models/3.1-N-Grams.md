## Section 3.1 N-Grams

P(w|h) is unable to give good predictions in most cases due to the limited size of corpus even if it is the web

probability of sentence or sequence of words:
 - P(W) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>...w<sub>n</sub>)

probability of an upcoming word:
 - P(w<sub>5</sub>|w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, w<sub>4</sub>)

a model that computes either of those is called a **language model**

Using the **chain rule**, the probability of the sequence of words can be expressed as:
 - P(W) = P(w<sub>1</sub>) * P(w<sub>2</sub>|w<sub>1</sub>) * P(w<sub>3</sub>|w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub>|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)

**bigram**: approximates the probability of a word given all the previous words by using only the conditional property of the preceding word
 - P(w<sub>n</sub>|w<sub>n-1</sub>)

**Markov**: the assumption that the probability of a word depends only on the previous word

**n-gram**:
 - P(w1:n) ≈ Π(k=1:n) P(wk | wk-1)
 - P(w_1:n) \approx \prod_{k=1}^{n} P(w_k | w_{k-1})
 