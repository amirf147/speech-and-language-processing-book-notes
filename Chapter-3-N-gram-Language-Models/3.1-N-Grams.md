## Section 3.1 N-Grams

P(w|h) is unable to give good predictions in most cases due to the limited size of corpus even if it is the web

probability of sentence or sequence of words:
 - P(W) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>...w<sub>n</sub>)

probability of an upcoming word:
 - P(w<sub>5</sub>|w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, w<sub>4</sub>)

a model that computes either of those is called a **language model**

Using the **chain rule**, the probability of the sequence of words can be expressed as:
 - P(W) = P(w<sub>1</sub>) * P(w<sub>2</sub>|w<sub>1</sub>) * P(w<sub>3</sub>|w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub>|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)

**bigram**: approximates the probability of a word given all the previous words by using only the conditional property of the preceding word
 - P(w<sub>n</sub>|w<sub>n-1</sub>)

**Markov assumption**: the assumption that the probability of a word depends only on the previous word

**n-gram**:

$$ P(w_{1:n}) \approx \prod_{k=1}^{n} P(w_k | w_{k-1}) $$

**Maximum Likelihood Estimation (MLE)**: a way to estimate bigram or n-gram probabilities. **normalizing** counts from a corpus.

example of MLE of bigram probability:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)} $$

simplifying:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})} $$

MLE n-gram parameter estimation:

$$ P(w_n | w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})} $$

the ratio above is called a **relative frequency**.

> "In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i.e., P(T|M))."

> **FOUNDATIONAL STATISTICS KNOWLEDGE BRUSH UP:** Probability vs. Likelihood, maximization of likelihood, and other statistics foundations can be found in the statistics file
