## Section 3.1 N-Grams

P(w|h) is unable to give good predictions in most cases due to the limited size of corpus even if it is the web

probability of sentence or sequence of words:
 - P(W) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>...w<sub>n</sub>)

probability of an upcoming word:
 - P(w<sub>5</sub>|w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, w<sub>4</sub>)

a model that computes either of those is called a **language model**

Using the **chain rule**, the probability of the sequence of words can be expressed as:
 - P(W) = P(w<sub>1</sub>) * P(w<sub>2</sub>|w<sub>1</sub>) * P(w<sub>3</sub>|w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub>|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)

**bigram**: approximates the probability of a word given all the previous words by using only the conditional property of the preceding word
 - P(w<sub>n</sub>|w<sub>n-1</sub>)

**Markov assumption**: the assumption that the probability of a word depends only on the previous word

**n-gram**:

$$ P(w_{1:n}) \approx \prod_{k=1}^{n} P(w_k | w_{k-1}) $$

**Maximum Likelihood Estimation (MLE)**: a way to estimate bigram or n-gram probabilities. **normalizing** counts from a corpus.

example of MLE of bigram probability:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)} $$

simplifying:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})} $$

MLE n-gram parameter estimation:

$$ P(w_n | w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})} $$

the ratio above is called a **relative frequency**.

> "In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i.e., P(T|M))."

> **FOUNDATIONAL STATISTICS KNOWLEDGE BRUSH UP:** Probability vs. Likelihood, maximization of likelihood, and other statistics foundations can be found in the Statistics-Probability-Review.md

## Gemini clarification:
Attempt at tying in the knowledge outlined the statistics probability review with gemini to understand maximum likelihood estimation:

 probability density functions (pdfs), likelihood, and maximum likelihood estimation (MLE) all play a role in estimating n-gram or bigram probabilities. Here's how they connect:

**N-gram Probabilities:**

* Represent the probability of a sequence of n words occurring in a specific order.
* Example: Bigram probability of "the cat" is the probability of encountering "cat" after seeing "the."

**MLE for N-gram Estimation:**

* MLE is a common technique for estimating n-gram probabilities from a text corpus (collection of text data).
* **Intuition:** We assume the observed data (text corpus) is a sample from the true underlying language model. MLE aims to find the n-gram probabilities that maximize the likelihood of observing the actual text corpus.

**Connecting MLE and PDFs:**

While we're not directly dealing with continuous distributions and pdfs in n-gram estimation, the concept of likelihood plays a similar role:

* In a continuous distribution, the pdf describes the probability density of different values for a variable.
* In n-gram estimation, we don't have a pdf. Instead, we're interested in the probability of specific n-word sequences (like "the cat").

**MLE in Action:**

1. **Count N-grams:** We count how many times each n-gram sequence appears in the text corpus.
2. **Normalization:** We normalize these counts by the total number of (n-1)-grams (for bigrams, the total number of words minus one). This essentially calculates the relative frequency of each n-gram compared to all possible n-gram sequences in the corpus.

**Why MLE and Not Probability:**

* We don't have a pre-defined "distribution" for n-gram sequences. The text corpus represents the observed data, and MLE helps us find the probabilities that best explain these observations.
* Unlike continuous distributions with infinitely many values, a specific n-gram sequence either occurs in the corpus or doesn't. MLE estimates the probability based on observed frequencies, not a continuous density function.

**Limitations of MLE:**

* MLE can overestimate probabilities of n-grams that rarely appear (seen only once or twice). Smoothing techniques are often used to address this issue.
* MLE assumes independence between n-grams, which isn't entirely true in language. Techniques like n-gram models with context (like trigrams) can help address this limitation.

**In essence, MLE provides a way to estimate n-gram probabilities by maximizing the likelihood of observing the actual text corpus based on the observed frequencies of n-gram sequences.** While not directly using pdfs, the concept of likelihood plays a crucial role in finding the most probable n-gram probabilities based on the training data. 
