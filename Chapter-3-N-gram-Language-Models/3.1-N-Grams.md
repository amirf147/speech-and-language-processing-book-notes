## Section 3.1 N-Grams

P(w|h) is unable to give good predictions in most cases due to the limited size of corpus even if it is the web

probability of sentence or sequence of words:
 - P(W) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>...w<sub>n</sub>)

probability of an upcoming word:
 - P(w<sub>5</sub>|w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, w<sub>4</sub>)

a model that computes either of those is called a **language model**

Using the **chain rule**, the probability of the sequence of words can be expressed as:
 - P(W) = P(w<sub>1</sub>) * P(w<sub>2</sub>|w<sub>1</sub>) * P(w<sub>3</sub>|w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub>|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)

**bigram**: approximates the probability of a word given all the previous words by using only the conditional property of the preceding word
 - P(w<sub>n</sub>|w<sub>n-1</sub>)

**Markov assumption**: the assumption that the probability of a word depends only on the previous word

**n-gram**:

$$ P(w_{1:n}) \approx \prod_{k=1}^{n} P(w_k | w_{k-1}) $$

**Maximum Likelihood Estimation (MLE)**: a way to estimate bigram or n-gram probabilities. **normalizing** counts from a corpus.

example of MLE of bigram probability:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)} $$

simplifying:

$$ P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})} $$

MLE n-gram parameter estimation:

$$ P(w_n | w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})} $$

the ratio above is called a **relative frequency**.

> "In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i.e., P(T|M))."

> **FOUNDATIONAL STATISTICS KNOWLEDGE BRUSH UP:** Probability vs. Likelihood, maximization of likelihood, and other statistics foundations can be found in the Statistics-Probability-Review.md

## Gemini clarification:
Attempt at tying in the knowledge outlined the statistics probability review with gemini to understand maximum likelihood estimation:

 probability density functions (pdfs), likelihood, and maximum likelihood estimation (MLE) all play a role in estimating n-gram or bigram probabilities. Here's how they connect:

**N-gram Probabilities:**

* Represent the probability of a sequence of n words occurring in a specific order.
* Example: Bigram probability of "the cat" is the probability of encountering "cat" after seeing "the."

**MLE for N-gram Estimation:**

* MLE is a common technique for estimating n-gram probabilities from a text corpus (collection of text data).
* **Intuition:** We assume the observed data (text corpus) is a sample from the true underlying language model. MLE aims to find the n-gram probabilities that maximize the likelihood of observing the actual text corpus.

**Connecting MLE and PDFs:**

While we're not directly dealing with continuous distributions and pdfs in n-gram estimation, the concept of likelihood plays a similar role:

* In a continuous distribution, the pdf describes the probability density of different values for a variable.
* In n-gram estimation, we don't have a pdf. Instead, we're interested in the probability of specific n-word sequences (like "the cat").

**MLE in Action:**

1. **Count N-grams:** We count how many times each n-gram sequence appears in the text corpus.
2. **Normalization:** We normalize these counts by the total number of (n-1)-grams (for bigrams, the total number of words minus one). This essentially calculates the relative frequency of each n-gram compared to all possible n-gram sequences in the corpus.

**Why MLE and Not Probability:**

* We don't have a pre-defined "distribution" for n-gram sequences. The text corpus represents the observed data, and MLE helps us find the probabilities that best explain these observations.
* Unlike continuous distributions with infinitely many values, a specific n-gram sequence either occurs in the corpus or doesn't. MLE estimates the probability based on observed frequencies, not a continuous density function.

**Limitations of MLE:**

* MLE can overestimate probabilities of n-grams that rarely appear (seen only once or twice). Smoothing techniques are often used to address this issue.
* MLE assumes independence between n-grams, which isn't entirely true in language. Techniques like n-gram models with context (like trigrams) can help address this limitation.

**In essence, MLE provides a way to estimate n-gram probabilities by maximizing the likelihood of observing the actual text corpus based on the observed frequencies of n-gram sequences.** While not directly using pdfs, the concept of likelihood plays a crucial role in finding the most probable n-gram probabilities based on the training data. 

### I then asked: can you expand on the intuition where you said that we assume observed data is a sample from the true underlying language model

The intuition behind assuming the observed data (text corpus) is a sample from the true underlying language model goes like this:

* **Imagine the Language:** Imagine there's a hidden "language model" that governs how sentences are formed in a specific language. This model defines the true probabilities of different word sequences. We don't directly observe this model, but its influence is reflected in the actual text written in that language.
* **Text Corpus as a Sample:** The text corpus you have is a collection of text samples generated by this underlying language model. It's like having a subset of sentences written according to the "rules" of the language model.
* **MLE as Finding the Best Fit:** Maximum likelihood estimation (MLE) aims to find the set of n-gram probabilities that best explains the observed text corpus. It's like trying to reverse engineer the underlying language model by analyzing the sample sentences (text corpus).

Here's a breakdown of the thought process:

1. **True Language Model:** There exists a true underlying language model that dictates the probabilities of different word sequences in a specific language. This model represents the "ground truth" you're trying to understand.
2. **Observed Data (Text Corpus):** You have a collection of text samples (sentences) generated by this language model. This text corpus represents a limited view of the true model.
3. **MLE as Inference:** MLE uses the observed data (text corpus) to infer the most likely set of n-gram probabilities that could have generated this data. It assumes that the observed data is a representative sample of the true language model's output.
4. **Maximizing Likelihood:** By calculating the likelihood of observing the actual text corpus under different n-gram probability settings, MLE identifies the set of probabilities that maximizes this likelihood. This essentially means finding the n-gram probabilities that best explain how the observed text corpus came to be.

**Think of it like this:**

Imagine you have a bag of colored marbles, but you can't see inside. You randomly draw some marbles (text corpus). MLE helps you estimate the proportion of each color (n-gram probabilities) in the entire bag based on the colors you drew (observed data). The assumption is that the drawn marbles are a fair representation of the colors in the whole bag (true language model).

**Limitations of the Assumption:**

It's important to acknowledge the limitations of this assumption:

* **Limited Sample:** The text corpus is just a sample, and the true language model might have more complex rules than what the sample reveals.
* **Independence Assumption:** MLE often assumes independence between n-grams, which isn't entirely true in language (the probability of "the cat" is different from "walking the cat").

**Despite these limitations, assuming the observed data is a sample from the true underlying language model provides a powerful starting point for MLE to estimate n-gram probabilities that can be used for various language processing tasks.**

### i rambled a bit about how i understand it and got this response:

**Underlying Model and Likelihood:**

* You're right; in MLE for n-grams, we **don't** directly know the underlying model that governs how the text was generated. We're trying to infer that model by analyzing the observed data (text corpus).
* MLE focuses on **likelihood**, which refers to how well a specific set of n-gram probabilities explains the observed text corpus. We don't calculate the actual probability of the corpus being generated by the true model (which we don't know).

**Maximizing Likelihood, Not Probability:**

* MLE doesn't search for universes with the highest **probability** of generating the observed text. Probability requires knowing the complete underlying model and its true n-gram probabilities.
* Instead, MLE searches for the set of n-gram probabilities that maximizes the **likelihood** of observing the actual text corpus. This essentially means finding the n-gram probabilities that best explain how the observed text came to be, given the limitations of our knowledge about the true model.

**Multiple Universes (Analogy):**

While the concept of "universes" isn't strictly used in MLE, it can be a helpful analogy:

* Imagine countless universes, each with a different set of n-gram probabilities.
* MLE doesn't literally search through these universes. Instead, it evaluates the likelihood of observing the text corpus under different n-gram probability settings (like evaluating how well the observed text fits in each universe).
* The set of n-gram probabilities that maximizes the likelihood is like finding the universe where the observed text appears most "natural" given the limited information we have (the text corpus).

**Probability vs. Likelihood:**

* **Probability:** Refers to the chance of a specific event happening within a known context (like the probability of flipping heads with a fair coin). In n-gram estimation, if we knew the true underlying language model, we could calculate the probability of observing a specific sequence like "the cat" based on its n-gram probabilities.
* **Likelihood:** Refers to how well a specific model or hypothesis explains observed data. In MLE for n-grams, we don't know the true model. We evaluate the likelihood of different n-gram probability settings explaining the observed text corpus.

**In essence, MLE for n-grams leverages the concept of likelihood to find the set of n-gram probabilities that best explains the observed text corpus, given the limitations of our knowledge about the true underlying language model. It doesn't search for universes with the highest probability, but rather the n-gram probabilities that maximize the likelihood of observing the actual text we have.**

### probabilities associated with n-grams can be considered as part of a **discrete distribution**
