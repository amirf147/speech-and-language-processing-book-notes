## Chapter 2: Regular Expressions, Text Normalization, Edit Distance

### Section 2.5: Word Tokenization
Tokenization is the task segmenting out the text into words

#### Section 2.5.1: Top-down (rule-based) tokenization
Need to consider commas, periods another punctuation inside and outside of words, numbers, prices, urls, email addresses.

**Clitic**:
    - ne'er, we're need to be expanded
    - it's the part of the word that doesn't make sense on its own it only makes sense when it's attached to another word

**Penn Treebank tokenization**:
    - released by the Linguistic Data Consortium (LDC)
    - separates out clitics, keeps hyphenated words together, separates punctuation, new lines between tokens
    - Input: "The San Francisco-based restaurant," they said, "doesn’t charge $10".
    - Output: " The San Francisco-based restaurant , " they said ," does n’t charge $ 10 " 

characters in chinese are called **hanzi** and using these character units is enough

languages like japanese and thai character unit is too small, **word segmentation** is required
 
#### Section 2.5.2: Byte-Pair Encoding: A Bottom-up Tokenization Algorithm