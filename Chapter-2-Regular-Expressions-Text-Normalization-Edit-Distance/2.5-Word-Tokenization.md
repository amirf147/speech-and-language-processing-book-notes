## Chapter 2: Regular Expressions, Text Normalization, Edit Distance

### Section 2.5: Word Tokenization
Tokenization is the task segmenting out the text into words

### Section 2.5.1: Top-down (rule-based) tokenization
Need to consider commas, periods another punctuation inside and outside of words, numbers, prices, urls, email addresses.

**Clitic**:
    - ne'er, we're need to be expanded
    - it's the part of the word that doesn't make sense on its own it only makes sense when it's attached to another word

**Penn Treebank tokenization**:
    - released by the Linguistic Data Consortium (LDC)
    - separates out clitics, keeps hyphenated words together, separates punctuation, new lines between tokens
    - Input: "The San Francisco-based restaurant," they said, "doesn’t charge $10".
    - Output: " The San Francisco-based restaurant , " they said ," does n’t charge $ 10 " 

characters in chinese are called **hanzi** and using these character units is enough

**morpheme** is a single unit of meaning

languages like japanese and thai character unit is too small, **word segmentation** is required
 
### Section 2.5.2: Byte-Pair Encoding: A Bottom-up Tokenization Algorithm

Most commonly used in large language models. Good for dealing with unknown words.

**Subwords**:
    - sub strings or morphemes
    - un-, likely, -est

**Two parts to a tokenization scheme**
1. Token learner
2. Token segmenter

**Byte-pair Encoding (BPE)**:
is a subword tokenization method used to handle large vocabularies and rare words in text processing. It iteratively merges the most frequent pairs of characters or character sequences into single units, effectively balancing between word-level and character-level representations. This allows for efficient handling of rare and out-of-vocabulary words by breaking them into smaller, more manageable subword units.
