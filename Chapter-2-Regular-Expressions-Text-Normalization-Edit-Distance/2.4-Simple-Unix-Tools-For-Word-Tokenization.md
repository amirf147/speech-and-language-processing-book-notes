### Chapter 2: Regular Expressions, Text Normalization, Edit Distance

#### Section 2.4: Simple Unix Tools for Word Tokenization

**The Three Tasks of Text Normalization**
 1. Tokenizing words
 2. Normalizing word formats
 3. Segmenting sentences

**Simple Unix Example**:
  - tr -sc 'A-Za-z' '\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
